___
**Instructions**

Congratulations and thank you for taking part in this Internal Training on Ridge and LASSO Regression. To test your abilities about Ridge and LASSO Regression concepts and techniques.

(For more details, you can follow this reference:: <https://github.com/HafizahIlma/ridge-vs-lasso>)
___
 
1. Under this type of Regression that is NOT a regularization technique is:
- [ ] Ridge Regression
- [ ] Ordinary Least Square Regression
- [ ] Least absolute shrinkage and selection operator Regression


2. The benefits of Regression Regulations include, EXCEPT:

- [ ] can overcome the problem of multicollinearity
- [ ] Reduces the value of bias in the train data
- [ ] Overcoming the Problem of Overfitting


3. Which of the following statements is true about Ridge and Lasso Regression
- [ ] Ridge can make the coefficient exactly 0
- [ ] Ridge is better than LASSO
- [ ] LASSO can make the coefficient exactly 0


___
**Instructions for number 4-8**

In this quiz, you will compare 3 regression methods namely Ordinary Least Square (OLS) Regression, Ridge Regression, and Least Absolute Shrinkage Selector Operator (LASSO) Regression using 'BostonHousing' data from the `mlbench` library, and name the data object 'BostonHousing' as `data`.

Below is a description of each variable that will be used in this quiz.

Housing data for 506 census tracts of Boston from the 1970 census.

`crim`	per capita crime rate by town

`nox`	nitric oxides concentration (parts per 10 million)

`rm`	average number of rooms per dwelling

`age`	proportion of owner-occupied units built prior to 1940

`dis`	weighted distances to five Boston employment centres

`b`	1000(B - 0.63)^2 where B is the proportion of blacks by town

`lstat`	percentage of lower status of the population

`medv` median value of owner-occupied homes in USD 1000's
___

# Data Preprocessing  

First remove the following variables `zn`,` indus`, `chas`,` rad`, `tax`, and` ptratio`. And take only 50 data by observing `451: 500`.

```
#your code here
```

## Data Partition 

First, split `data` into a data train and test data where the data train is the first 35 data and the data test is the last 15 data. Make it into 7 objects where:

`train` : Contains 35 initial data
`test2` : Contains the last 15 data
`test`  : Contains the last 15 data * without a target variable *.
`xtrain`: Contains` train` data in the form of `model.matrix` without intercepts
`ytrain`: Contains the target variable from the` train` data
`ytest` : Contains the target variable from the` test` data
`xtest` : Contains` test` data in the form of `model.matrix` without intercepts

To answer the questions below try to predict the variable **medv** based on the remaining variables with 3 regression methods namely OLS Regression, Ridge Regression, and LASSO Regression.

# OLS Regression

Use the `lm` function to predict the `medv` variable based on the remaining variables with the OLS Regression method.

```
#your code here
```

## Evidence of Overfitting

Do check whether the OLS Regression model is overfitting or not. To find out, try comparing the Mean Square Error (MSE) values of the data train and the test data.

- MSE Model of Data Train
```
#your code here
```

- MSE Model of Data Test
```
#your code here
```

4. What is value of the MSE Data Train and Data Test?

- [ ] 12.98 and 3.50
- [ ] 14.17 and 4.08
- [ ] 3.08 and 12.21


# Ridge Regression

The second method is the Ridge Regression method to predict the `medv` variable based on the remaining variables.

## Selection of Lambda for Ridge Regression

Selection of the best lambda for Ridge Regression, generate 100 random data first using the following syntax:

```
set.seed(100)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
```


## Building a Ridge Regression Model for 100 Lambda

Create the Ridge Regression model, using the `cv.glmnet` function from the` glmnet` library with the parameters `alpha = 0` and` lambda = lambdas_to_try`

```
#your code here
```

## Best Lambda of Ridge Regression

```
best_lambda_ridge <- 
best_lambda_ridge
```

5. What is the best lambda value formed by the Ridge Regression model?
- [ ] 0.61
- [ ] 1.07
- [ ] 0.19



## Build Ridge Regression Models with Best Lambda

Create the Ridge Regression model final, using the `glmnet` function from the` glmnet` library with the parameters `alpha = 0` and` lambda = best_lambda_ridge`

```
#your code here
```

## Coefficients of Ridge Regression Model 

Then try to display the results of the coefficients that have been generated by the model.

```
predict.glmnet(...,  type = 'coefficients')
```


# LASSO Regression

The third method used is LASSO Regression to predict the `medv` variable based on the remaining variables.

## Selection of Lambda for Ridge Regression

Selection of the best lambda for Ridge Regression, generate 100 random data first using the following syntax:

```
set.seed(100)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
```

## Build a LASSO Regression Model for 100 Lambda

Create the Ridge Regression model, using the `cv.glmnet` function from the` glmnet` library with the parameters `alpha = 1` and` lambda = lambdas_to_try`

```
#your code here
```

## Best Lambda of LASSO Regression

```
#your code here
```

6. What is the best lambda value formed by the LASSO Regression model?
- [ ] 0.17
- [ ] 1.07
- [ ] 0.71



## Build LASSO Regression Model with Best Lambda

Create the LASSO Regression model final, using the `glmnet` function from the` glmnet` library with the parameters `alpha = 1` and` lambda = best_lambda_lasso`.

```
#your code here
```

## Coefficients of LASSO Regression Model 

Then try to display the results of the coefficients that have been generated by the model.

```
#your code here
```


7. How many coefficients are generated by lasso regression (**excluding intercept**)?
- [ ] 5
- [ ] 6
- [ ] 4



# Model Comparison

Compare the Mean Square Error (MSE) values of each model between the test data and the data train.

**OLS Regression**

```
# MSE of Data Train and Data Test (OLS)
# your code here
```


**Ridge Regression**

```
# MSE of Data Train (Ridge)
ridge_pred <- predict(...,  newx = ...)
mse_train <-

# MSE of Data Test (Ridge)
ridge_pred <- predict(...,  newx = ...)
mse_test <-
```

**LASSO Regression**
```
# MSE of Data Train and Data Test (LASSO) 
# your code here
```

8. Based on the results of the comparison of the three regression models, which model is the best according to the MSE value?
- [ ] OLS
- [ ] LASSO
- [ ] Ridge


