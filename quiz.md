---
title: "quiz"
author: "Hafizah Ilma"
date: "11/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
___
**Instructions**

Hello, congratulations and thank you for taking part in this Internal Training on Ridge and LASSO Regression. To test your abilities about Ridge and LASSO Regression concepts and techniques.

(For more details you can see the following references: <https://github.com/HafizahIlma/ridge-vs-lasso>)
___
 
1. Under this type of Regression that is NOT a regularization technique is:
- [ ] Ridge Regression
- [ ] Ordinary Least Square Regression
- [ ] Least absolute shrinkage and selection operator Regression

<br>

2. The benefits of Regression Regulations include, EXCEPT:

- [ ] can overcome the problem of multicollinearity
- [ ] Reduces the value of bias in the train data
- [ ] Overcoming the Problem of Overfitting

<br>

3. Which of the following statements is true about Ridge and Lasso Regression
- [ ] Ridge can make the coefficient exactly 0
- [ ] Ridge is better than LASSO
- [ ] LASSO can make the coefficient exactly 0

<br>

___
**Instructions for number 4-8**

In this quiz, you will compare 3 regression methods namely Ordinary Least Square (OLS) Regression, Ridge Regression, and Least Absolute Shrinkage Selector Operator (LASSO) Regression using 'BostonHousing' data from the `mlbench` library.
___



# Libraries and Read Data Data

```{r}
library(caret)
library(glmnet)
library(mlbench)
```

```{r}
data("BostonHousing")
data <- BostonHousing
```

# Data Descriptions

Housing data for 506 census tracts of Boston from the 1970 census.

`crim`	per capita crime rate by town

`nox`	nitric oxides concentration (parts per 10 million)

`rm`	average number of rooms per dwelling

`age`	proportion of owner-occupied units built prior to 1940

`dis`	weighted distances to five Boston employment centres

`b`	1000(B - 0.63)^2 where B is the proportion of blacks by town

`lstat`	percentage of lower status of the population

`medv` median value of owner-occupied homes in USD 1000's


# Data Wrangling 

First remove the following variables `zn`,` indus`, `chas`,` rad`, `tax`, and` ptratio`. And take only 50 data by observing `451: 500`.

```{r}
data <- data[...:..., -c(...:... , ...:...)]
```

# Data Partition 

```{r}
train <- 
test <-

## Split Data for Ridge and LASSO model

xtrain <- 
ytrain <- 

test2 <- 
ytest <- 
xtest <- 
```

To answer the questions below try to predict the variable **medv** based on the remaining variables with 3 regression methods namely OLS Regression, Ridge Regression, and LASSO Regression.

# OLS Regression

Use the `lm` function to predict the `medv` variable based on the remaining variables with the OLS Regression method.

```{r}
ols <- 
summary(ols)
```

# Evidence of Overfitting

Do check whether the OLS Regression model is overfitting or not. To find out, try comparing the Mean Square Error (MSE) values of the data train and the test data.

- MSE Model of Data Train
```{r}
ols_pred <- predict(..., newdata = ...)
mean((...-...)^2)
```

- MSE Model of Data Test
```{r, warning=FALSE}
ols_pred <- predict(..., newdata = ...)
mean((...-...)^2)
```

<br>

4. Berapa nilai MSE Data Train dan Data Test?

- [ ] 12.98 and 3.50
- [ ] 14.17 and 4.08
- [ ] 3.08 and 12.21  

<br>


# Ridge Regression

The second method is the Ridge Regression method to predict the `medv` variable based on the remaining variables.

## Selection of Lambda for Ridge Regression

Selection of the best lambda for Ridge Regression, generate 100 random data first using the following syntax:

```{r}
set.seed(100)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
```


## Building a Ridge Regression Model for 100 Lambda

```{r, warning=F}
set.seed(100)
ridge_cv <- cv.glmnet(..., ..., alpha = 0, lambda = ...)
```

# Best Lambda of Ridge Regression
```{r}
best_lambda_ridge <- 
best_lambda_ridge
```

<br>

5. What is the best lambda value formed by the Ridge Regression model?
- [ ] 0.61
- [ ] 1.07
- [ ] 0.19

<br>

## Build Ridge Regression Models with Best Lambda

```{r}
ridge_mod <- glmnet(..., ..., alpha = 0, lambda = ...)
```

## Coefficients of Ridge Regression Model 

Then try removing the results of the coefficients that have been generated by the model.

```{r}
predict.glmnet(...,  type = 'coefficients')
```


# LASSO Regression

The third method used is LASSO Regression to predict the `medv` variable based on the remaining variables.

## Selection of Lambda for Ridge Regression

Selection of the best lambda for Ridge Regression, generate 100 random data first using the following syntax:

```{r}
set.seed(100)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
```

## Membangun Model LASSO Regression untuk 100 Lambda

```{r warning=FALSE}
set.seed(100)
lasso_cv <- cv.glmnet(..., ..., alpha = 1, lambda = ...)
```

## Best Lambda of LASSO Regression

```{r}
best_lambda_lasso <-
best_lambda_lasso
```

<br>

6. What is the best lambda value formed by the LASSO Regression model?
- [ ] 0.17
- [ ] 1.07
- [ ] 0.71

<br>

## Build Ridge Regression Models with Best Lambda

```{r}
lasso_mod <- glmnet(..., ..., alpha = 1, lambda = ...)
```

## Coefficients of LASSO Regression Model 

Then try removing the results of the coefficients that have been generated by the LASSO Regression model.

```{r}
predict.glmnet(..., type = 'coefficients')
```

<br>

7. How many coefficients are generated by lasso regression (**excluding intercept**)?
- [ ] 5
- [ ] 6
- [ ] 4

<br>


# Model Comparison

Compare the Mean Square Error (MSE) values of each model between the test data and the data train.

**OLS Regression**

```{r, warning=FALSE}
# MSE of Data Train (OLS)
ols_pred <- predict(..., newdata = ...)
mean((...-...)^2)

# MSE of Data Test (OLS)
ols_pred <- predict(..., newdata = ...)
mean((...-...)^2)
```


```{r,warning=FALSE}
# MSE of Data Train (Ridge)
ridge_pred <- predict(...,  newx = ...)
mean((...-...)^2)

# MSE of Data Test (Ridge)
ridge_pred <- predict(...,  newx = ...)
mean((...-...)^2)
```

```{r, warning=FALSE}
# MSE of Data Train (LASSO)
lasso_pred <- predict(...,  newx = ...)
mean((...-...)^2)

# MSE of Data Test (LASSO)
lasso_pred <- predict(...,  newx = ...)
mean((...-...)^2)
```

<br>

8. Based on the results of the comparison of the three regression models, which model is the best according to the MSE value?
- [ ] OLS
- [ ] LASSO
- [ ] Ridge

<br>
